<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Data Analysis with Pandas: Complete Tutorial (2026) | NexTool</title>
    <meta name="description" content="Learn Pandas from scratch. This complete 2026 tutorial covers DataFrame creation, CSV/Excel/JSON import, filtering, groupby, merge/join, pivot tables, missing data, time series, visualization with matplotlib, and performance tips.">
    <meta name="keywords" content="pandas tutorial, python pandas, pandas dataframe, pandas groupby, pandas merge, pandas pivot table, pandas read csv, pandas tutorial 2026, python data analysis, pandas time series, pandas matplotlib">
    <meta name="author" content="NexTool Team">
    <meta name="robots" content="index, follow">
    <link rel="canonical" href="https://nextool.app/blog/python-data-analysis-pandas.html">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Python Data Analysis with Pandas: Complete Tutorial (2026)">
    <meta property="og:description" content="Master Pandas for data analysis. DataFrames, filtering, groupby, merge, pivot tables, time series, visualization, and performance tips with real code examples.">
    <meta property="og:url" content="https://nextool.app/blog/python-data-analysis-pandas.html">
    <meta property="og:site_name" content="NexTool">
    <meta property="og:image" content="https://nextool.app/assets/images/blog/python-data-analysis-pandas-og.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="article:published_time" content="2026-02-14T11:00:00Z">
    <meta property="article:modified_time" content="2026-02-14T11:00:00Z">
    <meta property="article:author" content="NexTool Team">
    <meta property="article:section" content="Tutorial">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Pandas">
    <meta property="article:tag" content="Data Analysis">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Python Data Analysis with Pandas: Complete Tutorial (2026)">
    <meta name="twitter:description" content="Master Pandas: DataFrames, CSV import, filtering, groupby, merge, pivot tables, time series, matplotlib visualization, and performance tips.">
    <meta name="twitter:image" content="https://nextool.app/assets/images/blog/python-data-analysis-pandas-og.png">

    <!-- JSON-LD: Article -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "Python Data Analysis with Pandas: Complete Tutorial (2026)",
        "description": "Learn Pandas from scratch. This complete 2026 tutorial covers DataFrame creation, CSV/Excel/JSON import, filtering, groupby, merge/join, pivot tables, missing data, time series, visualization with matplotlib, and performance tips.",
        "image": "https://nextool.app/assets/images/blog/python-data-analysis-pandas-og.png",
        "author": {
            "@type": "Organization",
            "name": "NexTool Team",
            "url": "https://nextool.app"
        },
        "publisher": {
            "@type": "Organization",
            "name": "NexTool",
            "logo": {
                "@type": "ImageObject",
                "url": "https://nextool.app/assets/images/logo.png"
            }
        },
        "datePublished": "2026-02-14T11:00:00Z",
        "dateModified": "2026-02-14T11:00:00Z",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://nextool.app/blog/python-data-analysis-pandas.html"
        },
        "wordCount": 3500,
        "keywords": ["pandas tutorial", "python pandas", "pandas dataframe", "pandas groupby", "pandas merge", "pandas pivot table", "pandas read csv", "python data analysis", "pandas time series"]
    }
    </script>

    <!-- JSON-LD: BreadcrumbList -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [
            {
                "@type": "ListItem",
                "position": 1,
                "name": "Home",
                "item": "https://nextool.app"
            },
            {
                "@type": "ListItem",
                "position": 2,
                "name": "Blog",
                "item": "https://nextool.app/blog/"
            },
            {
                "@type": "ListItem",
                "position": 3,
                "name": "Pandas Tutorial",
                "item": "https://nextool.app/blog/python-data-analysis-pandas.html"
            }
        ]
    }
    </script>

    <!-- JSON-LD: FAQPage -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "FAQPage",
        "mainEntity": [
            {
                "@type": "Question",
                "name": "What is the difference between a Pandas Series and a DataFrame?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "A Series is a one-dimensional labeled array that can hold any data type (integers, strings, floats, objects). Think of it as a single column. A DataFrame is a two-dimensional labeled data structure with rows and columns, like a spreadsheet or SQL table. Each column in a DataFrame is a Series. When you select a single column from a DataFrame with df['column_name'], you get a Series. When you select multiple columns with df[['col1', 'col2']], you get a DataFrame."
                }
            },
            {
                "@type": "Question",
                "name": "How do I handle missing data in Pandas?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Pandas represents missing data as NaN (Not a Number) for numeric data and None or NaT (Not a Time) for datetime data. Use df.isna() to detect missing values and df.isna().sum() to count them per column. To remove rows with missing data, use df.dropna(). To fill missing values, use df.fillna(value) with a constant, df.fillna(method='ffill') for forward-fill, or df.fillna(df.mean()) to fill with column means. For more control, use df.interpolate() to estimate missing values based on surrounding data. Always explore why data is missing before choosing a strategy."
                }
            },
            {
                "@type": "Question",
                "name": "What is the difference between merge, join, and concat in Pandas?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "pd.concat() stacks DataFrames vertically (adding rows) or horizontally (adding columns). It does not match on keys. pd.merge() combines DataFrames by matching values in one or more columns, similar to SQL JOIN. It supports inner, left, right, and outer joins with the 'how' parameter. df.join() is a convenience method that merges on the index by default. In practice, use concat when you want to stack datasets with the same structure, and merge when you want to combine datasets based on a shared column like a customer ID or date."
                }
            },
            {
                "@type": "Question",
                "name": "How do I speed up slow Pandas operations?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Several strategies improve performance. First, use vectorized operations instead of loops: df['total'] = df['price'] * df['qty'] is far faster than iterating with iterrows(). Second, specify dtypes when reading data: pd.read_csv('file.csv', dtype={'id': 'int32', 'name': 'category'}) reduces memory usage. Third, use categorical dtype for columns with few unique values. Fourth, filter early to reduce the size of data you process. Fifth, use query() for complex filtering as it can be faster than boolean indexing on large DataFrames. For truly large datasets, consider Polars, DuckDB, or Dask, which handle out-of-memory data and parallelize operations."
                }
            },
            {
                "@type": "Question",
                "name": "How do I group data and calculate aggregates with Pandas?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Use the groupby() method to split data into groups, apply a function, and combine results. Basic syntax: df.groupby('column').agg_function(). For example, df.groupby('department')['salary'].mean() calculates average salary per department. Use .agg() for multiple aggregations: df.groupby('dept').agg({'salary': ['mean', 'max'], 'bonus': 'sum'}). Group by multiple columns with df.groupby(['year', 'dept']). Access groups with .groups or .get_group('name'). Use transform() to broadcast group results back to the original DataFrame size, and filter() to keep only groups meeting a condition."
                }
            },
            {
                "@type": "Question",
                "name": "Can Pandas read Excel files and what do I need to install?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "Yes. Use pd.read_excel('file.xlsx') to read Excel files and df.to_excel('output.xlsx') to write them. Pandas requires an engine library: openpyxl for .xlsx files (pip install openpyxl) or xlrd for legacy .xls files. You can read specific sheets with sheet_name='Sheet2' or sheet_name=0, read specific columns with usecols='A:D', and skip rows with skiprows=2. To read all sheets at once, use sheet_name=None, which returns a dictionary of DataFrames keyed by sheet name."
                }
            }
        ]
    }
    </script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <style>
        *, *::before, *::after { margin: 0; padding: 0; box-sizing: border-box; }
        :root { --bg:#050508;--surface:#0a0a0f;--surface-2:#1a1a24;--surface-3:#232330;--primary:#6366f1;--primary-hover:#818cf8;--accent:#a855f7;--accent-hover:#c084fc;--text:#e2e8f0;--text-secondary:#94a3b8;--text-muted:#64748b;--border:#1e1e2e;--success:#22c55e;--warning:#f59e0b;--error:#ef4444;--code-bg:#0d0d14;--font-sans:'Inter',-apple-system,BlinkMacSystemFont,'Segoe UI',sans-serif;--font-mono:'JetBrains Mono','Fira Code',monospace;--max-width:800px;--header-height:64px; }
        html { scroll-behavior:smooth;-webkit-text-size-adjust:100%; }
        body { font-family:var(--font-sans);background:var(--bg);color:var(--text);line-height:1.75;font-size:16px;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;overflow-x:hidden; }
        ::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:var(--bg)}::-webkit-scrollbar-thumb{background:var(--surface-3);border-radius:4px}::-webkit-scrollbar-thumb:hover{background:var(--text-muted)}
        .nav{position:fixed;top:0;left:0;right:0;height:var(--header-height);background:rgba(5,5,8,.85);backdrop-filter:blur(20px);-webkit-backdrop-filter:blur(20px);border-bottom:1px solid var(--border);z-index:1000;display:flex;align-items:center;justify-content:center}.nav-inner{width:100%;max-width:1200px;padding:0 24px;display:flex;align-items:center;justify-content:space-between}.nav-logo{display:flex;align-items:center;gap:10px;text-decoration:none;color:var(--text);font-weight:700;font-size:1.25rem}.nav-logo-icon{width:32px;height:32px;background:linear-gradient(135deg,var(--primary),var(--accent));border-radius:8px;display:flex;align-items:center;justify-content:center;font-size:.875rem;font-weight:800;color:#fff}.nav-links{display:flex;align-items:center;gap:28px;list-style:none}.nav-links a{color:var(--text-secondary);text-decoration:none;font-size:.9rem;font-weight:500;transition:color .2s}.nav-links a:hover{color:var(--text)}.nav-cta{background:var(--primary);color:#fff!important;padding:8px 20px;border-radius:8px;font-weight:600;transition:background .2s,transform .2s}.nav-cta:hover{background:var(--primary-hover);transform:translateY(-1px)}
        .article-wrapper{max-width:var(--max-width);margin:0 auto;padding:calc(var(--header-height) + 48px) 24px 80px}.breadcrumb{display:flex;align-items:center;gap:8px;margin-bottom:32px;font-size:.85rem;color:var(--text-muted);flex-wrap:wrap}.breadcrumb a{color:var(--text-secondary);text-decoration:none;transition:color .2s}.breadcrumb a:hover{color:var(--primary)}
        .article-header{margin-bottom:48px;padding-bottom:32px;border-bottom:1px solid var(--border)}.article-category{display:inline-block;background:rgba(168,85,247,.12);color:var(--accent);padding:4px 14px;border-radius:20px;font-size:.8rem;font-weight:600;text-transform:uppercase;letter-spacing:.5px;margin-bottom:16px}.article-title{font-size:clamp(2rem,5vw,3rem);font-weight:800;line-height:1.15;color:var(--text);margin-bottom:16px;letter-spacing:-.03em}.article-subtitle{font-size:1.2rem;color:var(--text-secondary);line-height:1.6;margin-bottom:24px}.article-meta{display:flex;align-items:center;gap:20px;color:var(--text-muted);font-size:.875rem;flex-wrap:wrap}.article-meta-item{display:flex;align-items:center;gap:6px}
        .toc{background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px 28px;margin-bottom:48px}.toc-title{font-size:.85rem;font-weight:700;color:var(--text-secondary);text-transform:uppercase;letter-spacing:.5px;margin-bottom:16px}.toc-list{list-style:none;counter-reset:toc}.toc-list li{counter-increment:toc;margin-bottom:8px}.toc-list li a{color:var(--text-secondary);text-decoration:none;font-size:.925rem;display:flex;align-items:baseline;gap:10px;transition:color .2s,padding-left .2s;padding:4px 0}.toc-list li a::before{content:counter(toc,decimal-leading-zero);color:var(--text-muted);font-size:.8rem;font-family:var(--font-mono);min-width:20px}.toc-list li a:hover{color:var(--primary);padding-left:4px}
        .article-content h2{font-size:1.75rem;font-weight:700;color:var(--text);margin-top:56px;margin-bottom:20px;letter-spacing:-.02em;line-height:1.3;padding-top:16px;border-top:1px solid var(--border)}.article-content h2:first-child{margin-top:0;padding-top:0;border-top:none}.article-content h3{font-size:1.3rem;font-weight:600;color:var(--text);margin-top:36px;margin-bottom:14px;line-height:1.35}.article-content h4{font-size:1.1rem;font-weight:600;color:var(--text-secondary);margin-top:28px;margin-bottom:12px}.article-content p{margin-bottom:20px;color:var(--text-secondary);line-height:1.8}.article-content a{color:var(--primary);text-decoration:none;border-bottom:1px solid transparent;transition:border-color .2s}.article-content a:hover{border-bottom-color:var(--primary)}.article-content strong{color:var(--text);font-weight:600}.article-content ul,.article-content ol{margin-bottom:20px;padding-left:24px;color:var(--text-secondary)}.article-content li{margin-bottom:10px;line-height:1.7}.article-content li::marker{color:var(--primary)}.article-content blockquote{border-left:3px solid var(--accent);background:var(--surface);padding:16px 24px;margin:28px 0;border-radius:0 8px 8px 0;font-style:italic;color:var(--text-secondary)}.article-content blockquote p:last-child{margin-bottom:0}.article-content hr{border:none;border-top:1px solid var(--border);margin:48px 0}
        .article-content pre{background:var(--code-bg);border:1px solid var(--border);border-radius:12px;padding:20px 24px;overflow-x:auto;margin:24px 0}.article-content pre code{font-family:var(--font-mono);font-size:.875rem;line-height:1.65;color:var(--text);background:none;padding:0;border-radius:0}.article-content code{font-family:var(--font-mono);font-size:.85em;background:var(--surface);color:var(--accent);padding:2px 8px;border-radius:5px}
        .info-box{background:rgba(99,102,241,.08);border:1px solid rgba(99,102,241,.2);border-radius:12px;padding:20px 24px;margin:28px 0}.info-box.warning{background:rgba(245,158,11,.08);border-color:rgba(245,158,11,.2)}.info-box.danger{background:rgba(239,68,68,.08);border-color:rgba(239,68,68,.2)}.info-box.success{background:rgba(34,197,94,.08);border-color:rgba(34,197,94,.2)}.info-box-title{font-weight:700;margin-bottom:8px;font-size:.9rem;display:flex;align-items:center;gap:8px}.info-box p{color:var(--text-secondary);font-size:.925rem;margin-bottom:0}
        .tool-grid{display:grid;grid-template-columns:repeat(auto-fill,minmax(220px,1fr));gap:16px;margin:28px 0}.tool-card{background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:20px;text-decoration:none;color:var(--text);transition:border-color .2s,transform .2s,box-shadow .2s;display:flex;flex-direction:column;gap:8px}.tool-card:hover{border-color:var(--primary);transform:translateY(-2px);box-shadow:0 8px 24px rgba(99,102,241,.1)}.tool-card-icon{font-size:1.5rem;margin-bottom:4px}.tool-card-name{font-weight:600;font-size:.95rem}.tool-card-desc{font-size:.825rem;color:var(--text-muted);line-height:1.5}
        .cta-box{background:linear-gradient(135deg,rgba(99,102,241,.1),rgba(168,85,247,.1));border:1px solid rgba(99,102,241,.25);border-radius:16px;padding:40px 32px;text-align:center;margin:48px 0}.cta-box h3{font-size:1.5rem;font-weight:700;margin-bottom:12px;color:var(--text)}.cta-box p{color:var(--text-secondary);margin-bottom:24px;max-width:500px;margin-left:auto;margin-right:auto}.cta-button{display:inline-flex;align-items:center;gap:8px;background:var(--primary);color:#fff;padding:14px 32px;border-radius:10px;text-decoration:none;font-weight:600;font-size:1rem;transition:background .2s,transform .2s,box-shadow .2s}.cta-button:hover{background:var(--primary-hover);transform:translateY(-2px);box-shadow:0 8px 32px rgba(99,102,241,.3)}.cta-button.secondary{background:transparent;border:1px solid var(--primary);color:var(--primary);margin-left:12px}.cta-button.secondary:hover{background:rgba(99,102,241,.1)}
        .faq-section{margin-top:56px;padding-top:32px;border-top:1px solid var(--border)}.faq-section h2{margin-top:0!important;padding-top:0!important;border-top:none!important}.faq-item{background:var(--surface);border:1px solid var(--border);border-radius:12px;margin-bottom:12px;overflow:hidden}.faq-question{width:100%;background:none;border:none;color:var(--text);padding:20px 24px;font-size:1rem;font-weight:600;text-align:left;cursor:pointer;display:flex;justify-content:space-between;align-items:center;font-family:var(--font-sans);transition:background .2s}.faq-question:hover{background:var(--surface-2)}.faq-question .icon{transition:transform .3s;font-size:1.25rem;color:var(--text-muted);flex-shrink:0;margin-left:16px}.faq-item.open .faq-question .icon{transform:rotate(45deg)}.faq-answer{max-height:0;overflow:hidden;transition:max-height .3s ease}.faq-answer-inner{padding:0 24px 20px;color:var(--text-secondary);line-height:1.7}
        .author-box{display:flex;align-items:center;gap:20px;background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin:48px 0}.author-avatar{width:64px;height:64px;background:linear-gradient(135deg,var(--primary),var(--accent));border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:1.5rem;font-weight:700;color:#fff;flex-shrink:0}.author-info h4{font-weight:600;margin-bottom:4px}.author-info p{color:var(--text-muted);font-size:.875rem;margin:0;line-height:1.5}
        .footer{border-top:1px solid var(--border);padding:48px 24px;text-align:center;color:var(--text-muted);font-size:.85rem}.footer-inner{max-width:1200px;margin:0 auto}.footer-links{display:flex;justify-content:center;gap:24px;margin-bottom:20px;flex-wrap:wrap}.footer-links a{color:var(--text-secondary);text-decoration:none;transition:color .2s}.footer-links a:hover{color:var(--primary)}
        @media(max-width:768px){.nav-links{display:none}.article-wrapper{padding:calc(var(--header-height) + 24px) 16px 60px}.article-title{font-size:1.75rem}.tool-grid{grid-template-columns:1fr}.cta-box{padding:28px 20px}.cta-button.secondary{margin-left:0;margin-top:12px}.author-box{flex-direction:column;text-align:center}.article-content pre{padding:16px;border-radius:8px}.article-content h2{font-size:1.4rem}.article-content h3{font-size:1.15rem}}
        @media(max-width:480px){.article-title{font-size:1.5rem}.toc{padding:18px 20px}}
    </style>
</head>
<body>

    <nav class="nav"><div class="nav-inner"><a href="/" class="nav-logo"><span class="nav-logo-icon">NT</span>NexTool</a><ul class="nav-links"><li><a href="/">Home</a></li><li><a href="/free-tools/">Free Tools</a></li><li><a href="/blog/" class="active">Blog</a></li><li><a href="/free-tools/pro-upgrade.html" class="nav-cta">Get Pro</a></li></ul></div></nav>

    <article class="article-wrapper">

        <div class="breadcrumb">
            <a href="/">Home</a> <span>/</span>
            <a href="/blog/">Blog</a> <span>/</span>
            <span>Pandas Tutorial</span>
        </div>

        <header class="article-header">
            <span class="article-category">Python</span>
            <h1 class="article-title">Python Data Analysis with Pandas: Complete Tutorial (2026)</h1>
            <p class="article-subtitle">A hands-on Pandas tutorial covering everything from reading CSV files to building pivot tables and time series analysis. Each concept includes copy-paste code with realistic data so you can follow along in a Jupyter notebook.</p>
            <div class="article-meta">
                <span class="article-meta-item">February 14, 2026</span>
                <span class="article-meta-item">30 min read</span>
                <span class="article-meta-item">NexTool Team</span>
            </div>
        </header>

        <div class="toc">
            <div class="toc-title">In This Tutorial</div>
            <ol class="toc-list">
                <li><a href="#setup">Setup and Installation</a></li>
                <li><a href="#dataframes">Creating DataFrames</a></li>
                <li><a href="#reading">Reading Data: CSV, Excel, JSON</a></li>
                <li><a href="#selecting">Selecting and Filtering Data</a></li>
                <li><a href="#groupby">GroupBy: Split-Apply-Combine</a></li>
                <li><a href="#merge">Merging, Joining, and Concatenating</a></li>
                <li><a href="#pivot">Pivot Tables and Reshaping</a></li>
                <li><a href="#missing">Handling Missing Data</a></li>
                <li><a href="#timeseries">Time Series Analysis</a></li>
                <li><a href="#visualization">Visualization with Matplotlib</a></li>
                <li><a href="#performance">Performance Tips</a></li>
                <li><a href="#tools">Related Developer Tools</a></li>
                <li><a href="#faq">Frequently Asked Questions</a></li>
            </ol>
        </div>

        <div class="article-content">

            <p>Pandas is the foundation of data analysis in Python. Whether you are cleaning messy CSV exports, building reports from database queries, or preparing features for a machine learning model, Pandas is almost certainly part of the workflow. It handles tabular data the way NumPy handles numerical arrays: fast, flexible, and with an API designed for real-world messiness.</p>

            <p>This tutorial walks through every core Pandas operation with working code. Each example uses realistic data so you can paste it directly into a Jupyter notebook or Python script and see results immediately.</p>

            <div class="info-box">
                <p><strong>Working with CSV data?</strong> The <a href="/free-tools/csv-editor.html">CSV Editor</a> lets you open, edit, sort, and filter CSV files directly in your browser. The <a href="/free-tools/csv-to-json.html">CSV to JSON Converter</a> transforms tabular data into JSON format for API consumption.</p>
            </div>

            <h2 id="setup">Setup and Installation</h2>

<pre><code># Install Pandas (and optional dependencies)
pip install pandas matplotlib openpyxl

# Import convention
import pandas as pd
import numpy as np</code></pre>

            <p>The standard import alias is <code>pd</code>. Every tutorial, documentation page, and Stack Overflow answer uses it. Do not deviate from this convention.</p>

            <h2 id="dataframes">Creating DataFrames</h2>

            <p>A DataFrame is a two-dimensional labeled data structure. Think of it as a spreadsheet with named columns and numbered (or labeled) rows.</p>

            <h3>From a Dictionary</h3>

<pre><code>import pandas as pd

df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales', 'Marketing'],
    'salary': [95000, 72000, 88000, 78000, 71000],
    'years': [5, 3, 7, 2, 4]
})

print(df)
#       name   department  salary  years
# 0    Alice  Engineering   95000      5
# 1      Bob    Marketing   72000      3
# 2  Charlie  Engineering   88000      7
# 3    Diana        Sales   78000      2
# 4      Eve    Marketing   71000      4</code></pre>

            <h3>From a List of Dictionaries</h3>

<pre><code># Each dict is a row - common when parsing API responses
records = [
    {'product': 'Widget A', 'price': 29.99, 'qty': 150},
    {'product': 'Widget B', 'price': 49.99, 'qty': 85},
    {'product': 'Widget C', 'price': 19.99, 'qty': 300},
]
df = pd.DataFrame(records)</code></pre>

            <h3>Quick Inspection</h3>

<pre><code>df.shape        # (5, 4) - rows, columns
df.dtypes       # data type of each column
df.info()       # column names, types, non-null counts
df.describe()   # statistical summary (count, mean, std, min, max)
df.head(3)      # first 3 rows
df.tail(2)      # last 2 rows
df.columns      # column names as an Index
df.index        # row labels</code></pre>

            <h2 id="reading">Reading Data: CSV, Excel, JSON</h2>

            <h3>CSV Files</h3>

<pre><code># Basic read
df = pd.read_csv('sales_data.csv')

# Specify column types (reduces memory, prevents type guessing errors)
df = pd.read_csv('sales_data.csv', dtype={
    'order_id': 'int32',
    'customer_id': 'int32',
    'product': 'category',
    'amount': 'float32'
})

# Parse dates automatically
df = pd.read_csv('sales_data.csv', parse_dates=['order_date'])

# Read specific columns
df = pd.read_csv('large_file.csv', usecols=['name', 'email', 'total'])

# Handle different delimiters
df = pd.read_csv('european_data.csv', sep=';', decimal=',')

# Read in chunks (for files too large for memory)
chunks = pd.read_csv('huge_file.csv', chunksize=10000)
for chunk in chunks:
    process(chunk)</code></pre>

            <h3>Excel Files</h3>

<pre><code># Requires: pip install openpyxl
df = pd.read_excel('report.xlsx')

# Read a specific sheet
df = pd.read_excel('report.xlsx', sheet_name='Q4 Data')

# Read specific columns and skip header rows
df = pd.read_excel('report.xlsx', usecols='A:D', skiprows=2)

# Read all sheets into a dict of DataFrames
all_sheets = pd.read_excel('report.xlsx', sheet_name=None)
# all_sheets['Sheet1'], all_sheets['Q4 Data'], etc.</code></pre>

            <h3>JSON Files</h3>

<pre><code># Standard JSON array of objects
df = pd.read_json('data.json')

# Nested JSON (needs normalization)
import json
with open('nested.json') as f:
    raw = json.load(f)

df = pd.json_normalize(raw['results'], record_path='orders',
                        meta=['customer_id', 'customer_name'])</code></pre>

            <p>If you need to convert between CSV and JSON formats for data pipeline work, the <a href="/free-tools/json-to-csv.html">JSON to CSV Converter</a> and <a href="/free-tools/csv-to-json.html">CSV to JSON Converter</a> handle the transformation in your browser without uploading data to any server.</p>

            <h3>Writing Data Back Out</h3>

<pre><code># CSV
df.to_csv('output.csv', index=False)

# Excel
df.to_excel('output.xlsx', index=False, sheet_name='Results')

# JSON
df.to_json('output.json', orient='records', indent=2)

# Clipboard (paste into spreadsheet)
df.to_clipboard(index=False)</code></pre>

            <h2 id="selecting">Selecting and Filtering Data</h2>

            <h3>Column Selection</h3>

<pre><code># Single column (returns Series)
df['name']

# Multiple columns (returns DataFrame)
df[['name', 'salary']]

# Dot notation (only for simple column names without spaces)
df.name</code></pre>

            <h3>Row Selection with loc and iloc</h3>

<pre><code># loc: label-based selection
df.loc[0]                    # row with index label 0
df.loc[0:2, 'name':'salary'] # rows 0-2, columns name through salary
df.loc[df['salary'] > 80000] # boolean filter

# iloc: integer position-based selection
df.iloc[0]        # first row
df.iloc[0:3]      # first 3 rows
df.iloc[:, 0:2]   # all rows, first 2 columns
df.iloc[-1]       # last row</code></pre>

            <h3>Filtering (Boolean Indexing)</h3>

<pre><code># Single condition
high_earners = df[df['salary'] > 80000]

# Multiple conditions (use & for AND, | for OR, ~ for NOT)
eng_senior = df[(df['department'] == 'Engineering') & (df['years'] >= 5)]

# Filter with isin()
target_depts = df[df['department'].isin(['Engineering', 'Sales'])]

# Filter with string methods
df[df['name'].str.startswith('A')]
df[df['name'].str.contains('li', case=False)]

# query() method (cleaner for complex conditions)
df.query('salary > 80000 and department == "Engineering"')
df.query('years >= @min_years')  # use @ for Python variables</code></pre>

            <h3>Adding and Modifying Columns</h3>

<pre><code># New column from calculation
df['annual_bonus'] = df['salary'] * 0.10

# Conditional column with np.where
df['seniority'] = np.where(df['years'] >= 5, 'Senior', 'Junior')

# Multiple conditions with np.select
conditions = [
    df['years'] >= 7,
    df['years'] >= 3,
    df['years'] >= 0
]
labels = ['Senior', 'Mid', 'Junior']
df['level'] = np.select(conditions, labels)

# Apply a function to a column
df['name_upper'] = df['name'].apply(str.upper)

# Drop columns
df = df.drop(columns=['name_upper'])</code></pre>

            <h2 id="groupby">GroupBy: Split-Apply-Combine</h2>

            <p>GroupBy splits data into groups based on column values, applies a function to each group, and combines the results. It is the Pandas equivalent of SQL's <code>GROUP BY</code>.</p>

<pre><code># Average salary per department
df.groupby('department')['salary'].mean()
# department
# Engineering    91500.0
# Marketing      71500.0
# Sales          78000.0

# Multiple aggregations
summary = df.groupby('department').agg(
    avg_salary=('salary', 'mean'),
    max_salary=('salary', 'max'),
    headcount=('name', 'count'),
    avg_years=('years', 'mean')
)

# Group by multiple columns
df.groupby(['department', 'seniority'])['salary'].mean()

# Custom aggregation with a lambda
df.groupby('department')['salary'].agg(
    lambda x: x.max() - x.min()
).rename('salary_range')</code></pre>

            <h3>Transform and Filter</h3>

<pre><code># transform: broadcast group result to original DataFrame shape
df['dept_avg'] = df.groupby('department')['salary'].transform('mean')
df['salary_vs_avg'] = df['salary'] - df['dept_avg']

# filter: keep only groups meeting a condition
large_depts = df.groupby('department').filter(lambda x: len(x) >= 2)</code></pre>

            <h2 id="merge">Merging, Joining, and Concatenating</h2>

            <h3>pd.merge() -- SQL-Style Joins</h3>

<pre><code>employees = pd.DataFrame({
    'emp_id': [1, 2, 3, 4],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'dept_id': [10, 20, 10, 30]
})

departments = pd.DataFrame({
    'dept_id': [10, 20, 40],
    'dept_name': ['Engineering', 'Marketing', 'Legal']
})

# Inner join (only matching rows)
pd.merge(employees, departments, on='dept_id', how='inner')

# Left join (keep all employees, NaN for unmatched departments)
pd.merge(employees, departments, on='dept_id', how='left')

# Right join (keep all departments)
pd.merge(employees, departments, on='dept_id', how='right')

# Outer join (keep everything)
pd.merge(employees, departments, on='dept_id', how='outer')

# Join on different column names
pd.merge(employees, departments,
         left_on='dept_id', right_on='dept_id')</code></pre>

            <h3>pd.concat() -- Stacking DataFrames</h3>

<pre><code># Vertical stack (add rows)
q1 = pd.DataFrame({'month': ['Jan', 'Feb', 'Mar'], 'revenue': [100, 120, 110]})
q2 = pd.DataFrame({'month': ['Apr', 'May', 'Jun'], 'revenue': [130, 140, 125]})

full_year = pd.concat([q1, q2], ignore_index=True)

# Horizontal stack (add columns)
pd.concat([df1, df2], axis=1)</code></pre>

            <p>For a quick visual comparison of your data before and after merging, the <a href="/free-tools/json-diff.html">JSON Diff</a> tool highlights structural differences between two datasets.</p>

            <h2 id="pivot">Pivot Tables and Reshaping</h2>

<pre><code>sales = pd.DataFrame({
    'date': ['2026-01-01', '2026-01-01', '2026-01-02', '2026-01-02'],
    'product': ['Widget', 'Gadget', 'Widget', 'Gadget'],
    'region': ['North', 'South', 'North', 'South'],
    'revenue': [500, 300, 450, 320]
})

# Pivot table: average revenue by product and region
pd.pivot_table(sales, values='revenue', index='product',
               columns='region', aggfunc='mean')

# With totals
pd.pivot_table(sales, values='revenue', index='product',
               columns='region', aggfunc='sum', margins=True)

# Melt: wide to long format
wide = pd.DataFrame({
    'name': ['Alice', 'Bob'],
    'math': [90, 85],
    'science': [88, 92],
    'english': [95, 78]
})

long = wide.melt(id_vars='name', var_name='subject', value_name='score')
# name    subject  score
# Alice   math     90
# Alice   science  88
# Alice   english  95
# Bob     math     85
# ...</code></pre>

            <h2 id="missing">Handling Missing Data</h2>

<pre><code>df = pd.DataFrame({
    'name': ['Alice', 'Bob', None, 'Diana'],
    'salary': [95000, np.nan, 88000, 78000],
    'bonus': [5000, 3000, np.nan, np.nan]
})

# Detect missing values
df.isna()          # Boolean DataFrame
df.isna().sum()    # Count NaN per column

# Drop rows with any NaN
df.dropna()

# Drop rows where specific columns are NaN
df.dropna(subset=['salary'])

# Drop columns with any NaN
df.dropna(axis=1)

# Fill with a constant
df['bonus'].fillna(0)

# Fill with column mean
df['salary'].fillna(df['salary'].mean())

# Forward-fill (use previous value)
df['salary'].fillna(method='ffill')

# Back-fill (use next value)
df['salary'].fillna(method='bfill')

# Interpolate (linear by default)
df['salary'].interpolate()

# Replace specific values
df.replace({'salary': {0: np.nan}})</code></pre>

            <div class="info-box">
                <p><strong>Strategy matters.</strong> Dropping rows with missing data is safe for exploratory analysis but can introduce bias in statistical models. Mean imputation preserves the average but reduces variance. Forward-fill works well for time series. Always understand why data is missing before choosing a strategy.</p>
            </div>

            <h2 id="timeseries">Time Series Analysis</h2>

<pre><code># Create a DatetimeIndex
dates = pd.date_range('2026-01-01', periods=365, freq='D')
ts = pd.DataFrame({
    'date': dates,
    'sales': np.random.randint(100, 500, size=365)
})
ts.set_index('date', inplace=True)

# Slice by date range
jan = ts['2026-01']
q1 = ts['2026-01':'2026-03']

# Resample: daily to monthly
monthly = ts.resample('M').sum()

# Resample: daily to weekly with multiple aggregations
weekly = ts.resample('W').agg({'sales': ['sum', 'mean', 'max']})

# Rolling average (7-day window)
ts['rolling_7d'] = ts['sales'].rolling(window=7).mean()

# Expanding (cumulative) sum
ts['cumulative'] = ts['sales'].expanding().sum()

# Shift (lag/lead)
ts['prev_day'] = ts['sales'].shift(1)    # yesterday's sales
ts['next_day'] = ts['sales'].shift(-1)   # tomorrow's sales

# Percentage change
ts['pct_change'] = ts['sales'].pct_change()

# Day of week, month extraction
ts['dow'] = ts.index.day_name()
ts['month'] = ts.index.month</code></pre>

            <h2 id="visualization">Visualization with Matplotlib</h2>

            <p>Pandas has built-in plotting that wraps Matplotlib. For quick exploration, it is faster than writing raw Matplotlib code.</p>

<pre><code>import matplotlib.pyplot as plt

# Line plot (great for time series)
ts['sales'].plot(figsize=(12, 4), title='Daily Sales')
plt.ylabel('Revenue ($)')
plt.tight_layout()
plt.savefig('daily_sales.png', dpi=150)
plt.show()

# Bar chart
df.groupby('department')['salary'].mean().plot.bar(
    color='#6366f1', title='Average Salary by Department'
)
plt.ylabel('Salary ($)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Histogram
df['salary'].plot.hist(bins=20, edgecolor='black',
                       title='Salary Distribution')
plt.xlabel('Salary ($)')
plt.show()

# Scatter plot
df.plot.scatter(x='years', y='salary', alpha=0.6,
                title='Experience vs Salary')
plt.show()

# Multiple subplots
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
df.groupby('department')['salary'].mean().plot.bar(ax=axes[0], title='Avg Salary')
df['years'].plot.hist(ax=axes[1], bins=10, title='Years Distribution')
plt.tight_layout()
plt.show()

# Box plot (detect outliers)
df.boxplot(column='salary', by='department', figsize=(8, 5))
plt.suptitle('')  # Remove automatic title
plt.title('Salary Distribution by Department')
plt.show()</code></pre>

            <p>When you need to fine-tune the colors in your charts, the <a href="/free-tools/color-picker.html">Color Picker</a> generates hex, RGB, and HSL values that you can drop directly into your Matplotlib style parameters.</p>

            <h2 id="performance">Performance Tips</h2>

            <h3>1. Use Vectorized Operations, Not Loops</h3>

<pre><code># Slow: iterating with iterrows
for idx, row in df.iterrows():
    df.at[idx, 'tax'] = row['salary'] * 0.3

# Fast: vectorized operation (100x+ faster on large datasets)
df['tax'] = df['salary'] * 0.3</code></pre>

            <h3>2. Optimize Data Types</h3>

<pre><code># Before: 304 bytes for a category column stored as object
df['department'].memory_usage(deep=True)

# After: convert to category (fewer unique values = big savings)
df['department'] = df['department'].astype('category')
df['department'].memory_usage(deep=True)  # significantly less

# Downcast numeric columns
df['salary'] = pd.to_numeric(df['salary'], downcast='integer')
df['years'] = pd.to_numeric(df['years'], downcast='integer')</code></pre>

            <h3>3. Filter Early</h3>

<pre><code># Slow: load everything, then filter
df = pd.read_csv('huge.csv')
filtered = df[df['region'] == 'North']

# Faster: only read what you need
df = pd.read_csv('huge.csv', usecols=['region', 'sales', 'date'])
filtered = df[df['region'] == 'North']</code></pre>

            <h3>4. Use query() for Complex Filters</h3>

<pre><code># Boolean indexing (creates temporary boolean arrays)
result = df[(df['salary'] > 80000) & (df['department'] == 'Engineering')]

# query() can be faster on large DataFrames
result = df.query('salary > 80000 and department == "Engineering"')</code></pre>

            <h3>5. Consider Alternatives for Big Data</h3>

            <ul>
                <li><strong>Polars</strong> -- a Rust-based DataFrame library that is 5-10x faster than Pandas for many operations, with a Pandas-like API.</li>
                <li><strong>DuckDB</strong> -- an in-process SQL database that can query Pandas DataFrames, Parquet files, and CSV files with SQL syntax, often faster than Pandas for aggregations.</li>
                <li><strong>Dask</strong> -- parallel Pandas that works on datasets larger than memory by splitting work across multiple cores.</li>
            </ul>

            <p>For formatting the JSON output from your Pandas data pipelines, the <a href="/free-tools/json-formatter.html">JSON Formatter</a> validates structure and formats output with consistent indentation.</p>

            <!-- Tools Section -->
            <h2 id="tools">Related Developer Tools</h2>

            <p>Free browser-based tools for working with the data formats you encounter in Pandas workflows.</p>

            <div class="tool-grid">
                <a href="/free-tools/csv-editor.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F4CA;</div>
                    <div class="tool-card-name">CSV Editor</div>
                    <div class="tool-card-desc">Open, edit, sort, and filter CSV files directly in your browser. No upload required.</div>
                </a>
                <a href="/free-tools/csv-to-json.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F504;</div>
                    <div class="tool-card-name">CSV to JSON Converter</div>
                    <div class="tool-card-desc">Transform CSV data into JSON format for API consumption or config files.</div>
                </a>
                <a href="/free-tools/json-to-csv.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F504;</div>
                    <div class="tool-card-name">JSON to CSV Converter</div>
                    <div class="tool-card-desc">Flatten JSON arrays into CSV format for import into spreadsheets or Pandas.</div>
                </a>
                <a href="/free-tools/json-formatter.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F4CB;</div>
                    <div class="tool-card-name">JSON Formatter</div>
                    <div class="tool-card-desc">Format, validate, and minify JSON. Essential for inspecting API responses and data exports.</div>
                </a>
                <a href="/free-tools/color-picker.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F3A8;</div>
                    <div class="tool-card-name">Color Picker</div>
                    <div class="tool-card-desc">Pick colors in hex, RGB, and HSL. Generate palettes for Matplotlib and Seaborn charts.</div>
                </a>
                <a href="/free-tools/sql-formatter.html" class="tool-card">
                    <div class="tool-card-icon">&#x1F5C3;</div>
                    <div class="tool-card-name">SQL Formatter</div>
                    <div class="tool-card-desc">Format and beautify SQL queries. Useful when converting between SQL and Pandas operations.</div>
                </a>
            </div>

            <hr>

            <!-- FAQ Section -->
            <div class="faq-section" id="faq">
                <h2>Frequently Asked Questions</h2>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>What is the difference between a Pandas Series and a DataFrame?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p>A Series is a one-dimensional labeled array that can hold any data type (integers, strings, floats, objects). Think of it as a single column. A DataFrame is a two-dimensional labeled data structure with rows and columns, like a spreadsheet or SQL table. Each column in a DataFrame is a Series. When you select a single column from a DataFrame with <code>df['column_name']</code>, you get a Series. When you select multiple columns with <code>df[['col1', 'col2']]</code>, you get a DataFrame.</p>
                        </div>
                    </div>
                </div>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>How do I handle missing data in Pandas?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p>Pandas represents missing data as NaN for numeric data and None or NaT for datetime data. Use <code>df.isna()</code> to detect missing values and <code>df.isna().sum()</code> to count them per column. To remove rows with missing data, use <code>df.dropna()</code>. To fill missing values, use <code>df.fillna(value)</code> with a constant, <code>df.fillna(method='ffill')</code> for forward-fill, or <code>df.fillna(df.mean())</code> to fill with column means. For more control, use <code>df.interpolate()</code> to estimate missing values based on surrounding data. Always explore why data is missing before choosing a strategy.</p>
                        </div>
                    </div>
                </div>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>What is the difference between merge, join, and concat in Pandas?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p><code>pd.concat()</code> stacks DataFrames vertically (adding rows) or horizontally (adding columns). It does not match on keys. <code>pd.merge()</code> combines DataFrames by matching values in one or more columns, similar to SQL JOIN. It supports inner, left, right, and outer joins with the <code>how</code> parameter. <code>df.join()</code> is a convenience method that merges on the index by default. In practice, use concat when you want to stack datasets with the same structure, and merge when you want to combine datasets based on a shared column like a customer ID or date.</p>
                        </div>
                    </div>
                </div>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>How do I speed up slow Pandas operations?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p>Several strategies improve performance. First, use vectorized operations instead of loops: <code>df['total'] = df['price'] * df['qty']</code> is far faster than iterating with <code>iterrows()</code>. Second, specify dtypes when reading data to reduce memory usage. Third, use categorical dtype for columns with few unique values. Fourth, filter early to reduce the size of data you process. Fifth, use <code>query()</code> for complex filtering as it can be faster than boolean indexing on large DataFrames. For truly large datasets, consider Polars, DuckDB, or Dask, which handle out-of-memory data and parallelize operations.</p>
                        </div>
                    </div>
                </div>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>How do I group data and calculate aggregates with Pandas?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p>Use the <code>groupby()</code> method to split data into groups, apply a function, and combine results. Basic syntax: <code>df.groupby('column').agg_function()</code>. For example, <code>df.groupby('department')['salary'].mean()</code> calculates average salary per department. Use <code>.agg()</code> for multiple aggregations: <code>df.groupby('dept').agg({'salary': ['mean', 'max'], 'bonus': 'sum'})</code>. Group by multiple columns with <code>df.groupby(['year', 'dept'])</code>. Use <code>transform()</code> to broadcast group results back to the original DataFrame size, and <code>filter()</code> to keep only groups meeting a condition.</p>
                        </div>
                    </div>
                </div>

                <div class="faq-item">
                    <button class="faq-question" onclick="toggleFAQ(this)">
                        <span>Can Pandas read Excel files and what do I need to install?</span>
                        <span class="icon">+</span>
                    </button>
                    <div class="faq-answer">
                        <div class="faq-answer-inner">
                            <p>Yes. Use <code>pd.read_excel('file.xlsx')</code> to read Excel files and <code>df.to_excel('output.xlsx')</code> to write them. Pandas requires an engine library: openpyxl for .xlsx files (<code>pip install openpyxl</code>) or xlrd for legacy .xls files. You can read specific sheets with <code>sheet_name='Sheet2'</code> or <code>sheet_name=0</code>, read specific columns with <code>usecols='A:D'</code>, and skip rows with <code>skiprows=2</code>. To read all sheets at once, use <code>sheet_name=None</code>, which returns a dictionary of DataFrames keyed by sheet name.</p>
                        </div>
                    </div>
                </div>
            </div>

        </div>

        <!-- Author -->
        <div class="author-box">
            <div class="author-avatar">NT</div>
            <div class="author-info">
                <h4>NexTool Team</h4>
                <p>We build free developer tools including CSV editors, JSON formatters, data converters, and 150+ more. All browser-based, no signup required.</p>
            </div>
        </div>

        <!-- Bottom CTA -->
        <div class="cta-box">
            <h3>150+ Developer Tools, One Place</h3>
            <p>NexTool Pro ($29) unlocks clean output, enhanced features, and unlimited workspace across every tool. One payment, lifetime access.</p>
            <a href="/free-tools/pro-upgrade.html" class="cta-button">Get NexTool Pro &mdash; $29</a>
            <a href="/free-tools/" class="cta-button secondary">Try Free Tools</a>
        </div>

    </article>

    <footer class="footer"><div class="footer-inner"><div class="footer-links"><a href="/">Home</a><a href="/free-tools/">Free Tools</a><a href="/blog/">Blog</a><a href="mailto:christianjunbucher@gmail.com">Contact</a></div><p>&copy; 2026 NexTool. All rights reserved. 227+ free developer tools.</p></div></footer>

    <script>
        function toggleFAQ(button) {
            const item = button.parentElement;
            const answer = item.querySelector('.faq-answer');
            const isOpen = item.classList.contains('open');
            document.querySelectorAll('.faq-item').forEach(faq => {
                faq.classList.remove('open');
                faq.querySelector('.faq-answer').style.maxHeight = null;
            });
            if (!isOpen) {
                item.classList.add('open');
                answer.style.maxHeight = answer.scrollHeight + 'px';
            }
        }
    </script>

    <script defer src="/js/analytics-lite.js"></script>
    <script defer src="/js/revenue.js"></script>
    <script defer src="/js/lead-capture.js"></script>
</body>
</html>